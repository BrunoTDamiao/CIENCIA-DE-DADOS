{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b0aef7-fda1-4d79-bdda-47ada239afaf",
   "metadata": {},
   "source": [
    "Tarefa 1 \n",
    "1. Cite 5 diferenças entre o Random Forest e o AdaBoost\n",
    "2. Acesse o link Scikit-learn– adaboost , leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost.\n",
    "3. Cite 5 Hyperparametros importantes no AdaBoost.\n",
    "4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f53950-8480-440d-adf8-978fe164be57",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o Random Forest e o AdaBoost\n",
    "\n",
    "#### Random Forest: \n",
    "> - Baseia-se em Bagging (Bootstrap Aggregating). Cada árvore é construída de forma independente em subconjuntos diferentes do dataset.\n",
    "> - Reduz a variância combinando previsões de várias árvores independentes e diversificadas.\n",
    "> - Todas as árvores são construídas em paralelo e de forma independente.\n",
    "> - Todas as árvores contribuem igualmente para a decisão final (votação por maioria ou média).\n",
    "> - Funciona bem em datasets ruidosos e com outliers, pois as árvores são independentes e menos sensíveis a erros individuais.\n",
    "\n",
    "#### AdaBoost:\n",
    "> - Usa Boosting, onde cada modelo é treinado sequencialmente, e os erros das árvores anteriores influenciam o treinamento das próximas.\n",
    "> - Reduz o viés dando maior peso às amostras mal classificadas, fazendo com que os modelos subsequentes se concentrem nesses erros.\n",
    "> - As árvores são construídas sequentemente, onde cada árvore se ajusta aos erros das anteriores, atualizando os pesos das amostras.\n",
    "> -  As árvores têm pesos diferentes na agregação final, com maior peso para aquelas que tiveram bom desempenho.\n",
    "> -  É mais sensível a ruído e outliers, já que o algoritmo dá maior peso a amostras mal classificadas, o que pode levar a um desempenho inferior em dados ruidosos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fac5a-4d8d-4440-99c0-7fcef868406e",
   "metadata": {},
   "source": [
    "### 2. Acesse o link Scikit-learn– adaboost , leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost.\n",
    "\n",
    "> - Bootstrap: Treina um classificador base (por exemplo, uma Árvore de Decisão) com o dataset original, mas dá mais peso aos exemplos mal classificados.\n",
    "> - Modelagem: Após cada iteração, o algoritmo ajusta os pesos das instâncias e treina um novo modelo focado nas amostras que foram mal classificadas anteriormente.\n",
    "> - Agregação: Combina todos os modelos treinados em um ensemble, ponderando a decisão de cada um com base na performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6ab896-adc4-45e7-b2ed-74d79ea798b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.983"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)\n",
    "AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.predict([[0, 0, 0, 0]])\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526f36a-08b9-465f-8c6a-b21c93854ab0",
   "metadata": {},
   "source": [
    "### 3. Cite 5 Hyperparametros importantes no AdaBoost.\n",
    "\n",
    "1. n_estimators\n",
    "> - Define o número de classificadores fracos (estimadores) a serem treinados sequencialmente.\n",
    "\n",
    "2. learning_rate\n",
    "> - Controla o peso atribuído a cada classificador fraco.\n",
    "\n",
    "3. base_estimator\n",
    "> - O tipo de modelo usado como classificador fraco, por exemplo, uma árvore de decisão rasa (DecisionTreeClassifier com max_depth=1).\n",
    "\n",
    "4. algorithm\n",
    "> - Pode ser 'SAMME' (para classificação multi-classe) ou 'SAMME.R' (mais eficiente, usa probabilidades).\n",
    "\n",
    "5. random_state\n",
    "> - Controla a semente aleatória para garantir reprodutibilidade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3dbe5-976b-483a-b878-b619a2d3719d",
   "metadata": {},
   "source": [
    "### 4. Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bc1b687-f475-4165-8a95-1fc9319ce810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Melhores parâmetros: {'algorithm': 'SAMME', 'estimator__max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 10}\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Carregando o conjunto de dados Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Dividindo o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Configurando o classificador base (DecisionTreeClassifier)\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Definindo os hiperparâmetros para o GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],  # Número de estimadores\n",
    "    'learning_rate': [0.01, 0.1, 1.0],   # Taxa de aprendizado\n",
    "    'estimator__max_depth': [1, 2, 3],  # Profundidade máxima da árvore base\n",
    "    'algorithm': ['SAMME', 'SAMME.R']  # Tipo de algoritmo\n",
    "}\n",
    "\n",
    "# Instanciando o AdaBoost com a árvore como base\n",
    "ada = AdaBoostClassifier(estimator=estimator, random_state=42)\n",
    "\n",
    "# Configurando o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=ada, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Treinando o modelo com o GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Exibindo os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros:\", grid_search.best_params_)\n",
    "\n",
    "# Fazendo previsões no conjunto de teste\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Exibindo o relatório de classificação\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71439c-3e73-4dfd-a621-9ba2a7ad7615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
