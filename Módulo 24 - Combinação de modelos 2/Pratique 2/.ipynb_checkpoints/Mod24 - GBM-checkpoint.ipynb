{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarefa 2\n",
    "1. Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "2. Acesse o link Scikit-learn – GBM, leia a explicação\n",
    "(traduza se for preciso) e crie um jupyter notebook\n",
    "contendo o exemplo de classificação e de regressão\n",
    "do GBM.\n",
    "3. Cite 5 Hyperparametros importantes no GBM.\n",
    "4. (Opcional) Utilize o GridSearch para encontrar os\n",
    "melhores hyperparametros para o conjunto de dados\n",
    "do exemplo\n",
    "5. Acessando o artigo do Jerome Friedman (Stochastic) e\n",
    "pensando no nome dado ao Stochastic GBM, qual é a\n",
    "maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "\n",
    "> ADABOOST\n",
    ">> - Em cada iteração, ele ajusta os pesos das amostras mal classificadas, forçando o próximo modelo a focar nessas instâncias problemáticas.\n",
    ">> - Mais simples e mais rápido de treinar, pois usa um único modelo fraco em cada passo.\n",
    ">> - Sensível a outliers, pois as instâncias mal classificadas recebem pesos altos e podem dominar o treinamento.\n",
    ">> - Focado em problemas de classificação binária ou multi-classe.\n",
    ">> - Mais interpretável devido à simplicidade do modelo e ao uso de estimadores fracos como árvores rasas.\n",
    "\n",
    "\n",
    "> GBM\n",
    ">> - Em vez de reponderar amostras, ele minimiza o residual do erro de predições anteriores, adicionando modelos que corrigem gradualmente os erros.\n",
    ">> -  Mais complexo e mais lento, pois cada modelo é treinado com base no gradiente dos erros anteriores, aumentando o custo computacional.\n",
    ">> -  Menos sensível, já que ele minimiza o erro ao longo de várias iterações, diluindo o impacto de pontos atípicos.\n",
    ">> -  Flexível para classificação e regressão, usando diferentes funções de perda (como erro quadrático para regressão e log-loss para classificação).\n",
    ">> -  Pode produzir resultados mais precisos, mas é mais difícil de interpretar devido à complexidade dos ajustes baseados em gradiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### classificação\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4379326910967305"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### regressão\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test[1:2])\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cite 5 Hyperparametros importantes no GBM.\n",
    "\n",
    "1. n_estimators:\n",
    "> - Define o número total de árvores (ou modelos fracos) que serão treinados.\n",
    "\n",
    "2. learning_rate:\n",
    "> - Controla a contribuição de cada árvore para a previsão final, reduzindo o impacto de cada estimador.\n",
    "\n",
    "3. max_depth:\n",
    "> - Controla a complexidade de cada árvore individual.\n",
    "\n",
    "4. subsample:\n",
    "> - Define a proporção dos dados de treino a ser usada para cada árvore.\n",
    "\n",
    "5. loss:\n",
    "> - Especifica a função de perda a ser minimizada. Por exemplo, squared_error é usada para regressão e deviance (log-loss) para classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Melhores parâmetros: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Acurácia com os melhores parâmetros: 0.9429\n",
      "Acurácia no conjunto de teste: 1.0000\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Carregar o conjunto de dados Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instanciar o modelo base\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Definir a grade de hiperparâmetros para testar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Instanciar o GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbm, \n",
    "    param_grid=param_grid, \n",
    "    cv=3,  # Validação cruzada com 3 divisões\n",
    "    n_jobs=-1,  # Paralelização\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Executar a busca pelos melhores parâmetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Exibir os melhores parâmetros e a acurácia correspondente\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f\"Acurácia com os melhores parâmetros: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Avaliar o desempenho no conjunto de teste\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Acurácia no conjunto de teste: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?\n",
    "> A maior diferença entre o Stochastic Gradient Boosting (SGB) e o Gradient Boosting Machine (GBM) convencional está na introdução da aleatoriedade durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
